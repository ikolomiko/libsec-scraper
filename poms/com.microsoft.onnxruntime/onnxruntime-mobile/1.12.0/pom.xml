<?xml version="1.0" encoding="UTF-8"?>
<project xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd" xmlns="http://maven.apache.org/POM/4.0.0"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
  <modelVersion>4.0.0</modelVersion>
  <groupId>com.microsoft.onnxruntime</groupId>
  <artifactId>onnxruntime-mobile</artifactId>
  <version>1.12.0</version>
  <packaging>aar</packaging>
  <name>onnx-runtime</name>
  <description>The ONNX Runtime Mobile package is a size optimized inference library for executing ONNX (Open Neural Network Exchange) models on Android. This package is built from the open source inference engine but with reduced disk footprint targeting mobile platforms. To minimize binary size this library supports a reduced set of operators and types aligned to typical mobile applications. The ONNX model must be converted to ORT format in order to use it with this package. See https://onnxruntime.ai/docs/reference/ort-model-format.html for more details.</description>
  <url>https://microsoft.github.io/onnxruntime/</url>
  <organization>
    <name>Microsoft</name>
    <url>http://www.microsoft.com</url>
  </organization>
  <licenses>
    <license>
      <name>MIT License</name>
      <url>https://opensource.org/licenses/MIT</url>
    </license>
  </licenses>
  <developers>
    <developer>
      <id>onnxruntime</id>
      <name>ONNX Runtime</name>
      <email>onnxruntime@microsoft.com</email>
    </developer>
  </developers>
  <scm>
    <connection>scm:git:git://github.com:microsoft/onnxruntime.git</connection>
    <developerConnection>scm:git:ssh://github.com/microsoft/onnxruntime.git</developerConnection>
    <url>http://github.com/microsoft/onnxruntime</url>
  </scm>
</project>
