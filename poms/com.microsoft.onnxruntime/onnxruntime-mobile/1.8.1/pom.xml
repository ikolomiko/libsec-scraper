<?xml version="1.0" encoding="UTF-8"?>
<project xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd" xmlns="http://maven.apache.org/POM/4.0.0"
    xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance">
  <modelVersion>4.0.0</modelVersion>
  <groupId>com.microsoft.onnxruntime</groupId>
  <artifactId>onnxruntime-mobile</artifactId>
  <version>1.8.1</version>
  <packaging>aar</packaging>
  <name>onnx-runtime</name>
  <description>ONNX Runtime Mobile package is a size optimized inference library for executing ONNX (Open Neural Network Exchange) models. This package is built from the open source inference engine targeting reduced disk footprint for mobile platforms.</description>
  <url>https://microsoft.github.io/onnxruntime/</url>
  <organization>
    <name>Microsoft</name>
    <url>http://www.microsoft.com</url>
  </organization>
  <licenses>
    <license>
      <name>MIT License</name>
      <url>https://opensource.org/licenses/MIT</url>
    </license>
  </licenses>
  <developers>
    <developer>
      <id>onnxruntime</id>
      <name>ONNX Runtime</name>
      <email>onnxruntime@microsoft.com</email>
    </developer>
  </developers>
  <scm>
    <connection>scm:git:git://github.com:microsoft/onnxruntime.git</connection>
    <developerConnection>scm:git:ssh://github.com/microsoft/onnxruntime.git</developerConnection>
    <url>http://github.com/microsoft/onnxruntime</url>
  </scm>
</project>
